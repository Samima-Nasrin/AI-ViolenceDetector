{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4e67a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python torch torchvision tqdm\n",
    "\n",
    "\n",
    "Load Dataset\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"DanJoshua/RWF-2000\")\n",
    "\n",
    "print(ds)\n",
    "\n",
    "Convert raw bytes to frames\n",
    "\n",
    "!pip install av\n",
    "\n",
    "import io\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Utility: Convert AVI bytes -> frames\n",
    "def avi_bytes_to_frames(avi_bytes, max_frames=32, every_n=5):\n",
    "    container = av.open(io.BytesIO(avi_bytes))\n",
    "    frames = []\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i % every_n == 0:\n",
    "            frames.append(np.array(frame.to_image()))\n",
    "        if len(frames) >= max_frames:\n",
    "            break\n",
    "    if len(frames) == 0:\n",
    "        return np.zeros((1, 112, 112, 3), dtype=np.uint8)\n",
    "    return np.stack(frames)\n",
    "\n",
    "#Utility: Resize frames to fixed size\n",
    "def resize_frames(frames, size=(112,112)):\n",
    "    resized = [cv2.resize(f, size) for f in frames]\n",
    "    return np.stack(resized)\n",
    "\n",
    "#PyTorch Dataset\n",
    "class RWF2000Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, max_frames=32, every_n=5, frame_size=(112,112)):\n",
    "        self.dataset = hf_dataset\n",
    "        self.max_frames = max_frames\n",
    "        self.every_n = every_n\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        avi_bytes = item[\"avi\"]\n",
    "        frames = avi_bytes_to_frames(avi_bytes, self.max_frames, self.every_n)\n",
    "        frames = resize_frames(frames, self.frame_size)\n",
    "        # Infer label: 0 = Non-Violence, 1 = Violence (based on file name)\n",
    "        label = 1 if \"Violence\" in item[\"__url__\"] else 0\n",
    "\n",
    "        # Convert to tensor (T, C, H, W)\n",
    "        frames_tensor = torch.from_numpy(frames).permute(0,3,1,2).float()\n",
    "        return frames_tensor, torch.tensor(label).long()\n",
    "\n",
    "#Instantiate Dataset + DataLoader\n",
    "train_dataset = RWF2000Dataset(ds[\"train\"], max_frames=32, every_n=5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "#Verify batch shapes\n",
    "for X, y in train_loader:\n",
    "    print(\"Video batch shape:\", X.shape)  # (B, T, C, H, W)\n",
    "    print(\"Labels shape:\", y.shape)       # (B,)\n",
    "    break\n",
    "\n",
    "CNN+LSTM\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_features=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc = nn.Linear(64*14*14, out_features)  # assuming 112x112 input\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)  # now size (B, 64, 14, 14)\n",
    "        x = x.reshape(x.size(0), -1)  # flatten\n",
    "        x = self.fc(x)\n",
    "        return x  # (B, out_features)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_out=256, lstm_hidden=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.cnn = CNNEncoder(out_features=cnn_out)\n",
    "        self.lstm = nn.LSTM(input_size=cnn_out, hidden_size=lstm_hidden,\n",
    "                            num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.size()\n",
    "        # Flatten batch & time to feed CNN\n",
    "        x = x.reshape(B*T, C, H, W)\n",
    "        features = self.cnn(x)  # (B*T, cnn_out)\n",
    "        # Restore sequence: (B, T, cnn_out)\n",
    "        features = features.view(B, T, -1)\n",
    "        # LSTM\n",
    "        out, (h_n, c_n) = self.lstm(features)\n",
    "        # Take last hidden state\n",
    "        last = h_n[-1]  # (B, lstm_hidden)\n",
    "        out = self.fc(last)  # (B, num_classes)\n",
    "        return out\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNN_LSTM().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "video_bytes = ds['train'][0]['avi']\n",
    "\n",
    "# Preprocess it the same way as during training\n",
    "def preprocess_video_bytes(avi_bytes):\n",
    "    frames = avi_bytes_to_frames(avi_bytes, max_frames=32, every_n=5)\n",
    "    frames = resize_frames(frames, size=(112,112))\n",
    "    frames_tensor = torch.from_numpy(frames).permute(0,3,1,2).float().unsqueeze(0)  # Add batch dim\n",
    "    return frames_tensor\n",
    "\n",
    "X = preprocess_video_bytes(video_bytes).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = torch.argmax(model(X), dim=1).item()\n",
    "\n",
    "print(\"Prediction:\", \"Violence\" if pred==1 else \"Non-Violence\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
